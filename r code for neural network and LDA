library(tidyverse)
library(GGally)
library(glmnet)
library(dplyr)
library(readxl)
library(reticulate)
library(keras)
library(MASS)

# Load data
df <- read.csv("gym_members_exercise_tracking.csv")

# Convert character variables to factors and then numeric
df$Workout_Type <- factor(df$Workout_Type, levels = c("Cardio", "HIIT", "Strength", "Yoga"), labels = c(1, 2, 3, 4))
df$Workout_Type <- as.numeric(df$Workout_Type)
df$Gender <- factor(df$Gender, levels = c("Female", "Male"), labels = c(1, 2))
df$Gender <- as.numeric(df$Gender)

# Calculate LBM, FFMI, and natural status
df$Fat_Mass <- df$Weight..kg. * (df$Fat_Percentage / 100)
df$LBM <- df$Weight..kg. - df$Fat_Mass
df$FFMI <- df$LBM / (df$Height..m.^2)

df <- df %>%
  mutate(natural_status = case_when(
    Gender == 2 & (LBM > 90 | FFMI > 25) ~ "PEDs",
    Gender == 2 & (LBM > 85 | FFMI > 23) ~ "Suspicious",
    Gender == 1 & (LBM > 70 | FFMI > 24) ~ "PEDs",
    Gender == 1 & (LBM > 65 | FFMI > 22) ~ "Suspicious",
    TRUE ~ "Natural"
  ))

df$natural_status <- factor(df$natural_status, levels = c("PEDs", "Suspicious", "Natural"), labels = c(1, 2, 3))
df$natural_status <- as.numeric(df$natural_status)
df <- df %>% dplyr::select(-LBM, -FFMI)


# Drop unnecessary variables
df <- df %>% dplyr::select(-Calories_Burned, -Water_Intake..liters., -Experience_Level)


# ======================
#  NEW TARGET: Workout_Type
# ======================

# Prepare data for PCA â€” Exclude Workout_Type from X
x <- as.matrix(df[, c("Age", "Gender", "Weight..kg.", "Height..m.", "Max_BPM", "Avg_BPM",
                      "Resting_BPM", "Session_Duration..hours.", "Fat_Percentage",
                      "Workout_Frequency..days.week.", "natural_status", "Fat_Mass", "BMI")])

y <- df$Workout_Type  # Set dependent variable to Workout_Type

# PCA
pca_model <- prcomp(x, scale. = TRUE)
pca_model$rotation <- -pca_model$rotation
pca_model$x <- -pca_model$x

# Prepare PCA dataset
pca_data <- data.frame(pca_model$x[, 1:7])
pca_data$target <- y
pca_data$target <- as.factor(pca_data$target)
pca_data$target_index <- as.integer(pca_data$target) - 1

# Prepare data for keras classification
X_pca <- as.matrix(pca_data[, paste0("PC", 1:7)])
y_pca <- pca_data$target_index

set.seed(13)
n <- nrow(X_pca)
train_size <- floor(0.6 * n)
val_size <- floor(0.2 * n)
test_size <- n - train_size - val_size

indices <- sample(1:n)
train_idx <- indices[1:train_size]
val_idx <- indices[(train_size + 1):(train_size + val_size)]
test_idx <- indices[(train_size + val_size + 1):n]

keras <- import("keras")
np <- import("numpy")
tuple <- import_builtins()$tuple
shape <- tuple(list(ncol(X_pca)))

x_train <- r_to_py(X_pca[train_idx, , drop = FALSE])
y_train <- r_to_py(np$reshape(y_pca[train_idx], tuple(list(length(train_idx), 1L))))

x_val <- r_to_py(X_pca[val_idx, , drop = FALSE])
y_val <- r_to_py(np$reshape(y_pca[val_idx], tuple(list(length(val_idx), 1L))))
val_data <- tuple(list(x_val, y_val))

x_test <- r_to_py(X_pca[test_idx, , drop = FALSE])
y_test <- r_to_py(np$reshape(y_pca[test_idx], tuple(list(length(test_idx), 1L))))

# Define model
model_pca <- keras$models$Sequential()
model_pca$add(keras$layers$Dense(units = 64L, activation = "relu", input_shape = shape))
model_pca$add(keras$layers$Dense(units = 32L, activation = "relu"))
model_pca$add(keras$layers$Dense(units = 4L, activation = "softmax"))  # 4 classes for workout type

# Compile
model_pca$compile(
  loss = "sparse_categorical_crossentropy",
  optimizer = keras$optimizers$Adam(),
  metrics = list("accuracy", "mae")
)

# Train
history_pca <- model_pca$fit(
  x = x_train,
  y = y_train,
  epochs = 150L,
  batch_size = 16L,
  validation_data = val_data
)

# Evaluate
pred_probs <- model_pca$predict(x_test)
pred_classes <- np$argmax(pred_probs, axis = 1L)
true_classes <- as.integer(py_to_r(y_test))

accuracy <- mean(pred_classes == true_classes)
mae_manual <- mean(abs(pred_classes - true_classes))

cat("PCA Test Accuracy:", round(accuracy * 100, 2), "%\n") #29.59%
cat("PCA Test MAE:", round(mae_manual, 4), "\n") # 1.3163
print(table(Predicted = pred_classes, Actual = true_classes))
# Prepare data for keras classification
X_pca <- as.matrix(pca_data[, paste0("PC", 1:7)])
y_pca <- pca_data$target_index

set.seed(13)
n <- nrow(X_pca)
train_size <- floor(0.6 * n)
val_size <- floor(0.2 * n)
test_size <- n - train_size - val_size

indices <- sample(1:n)
train_idx <- indices[1:train_size]
val_idx <- indices[(train_size + 1):(train_size + val_size)]
test_idx <- indices[(train_size + val_size + 1):n]

keras <- import("keras")
np <- import("numpy")
tuple <- import_builtins()$tuple
shape <- tuple(list(ncol(X_pca)))

x_train <- r_to_py(X_pca[train_idx, , drop = FALSE])
y_train <- r_to_py(np$reshape(y_pca[train_idx], tuple(list(length(train_idx), 1L))))

x_val <- r_to_py(X_pca[val_idx, , drop = FALSE])
y_val <- r_to_py(np$reshape(y_pca[val_idx], tuple(list(length(val_idx), 1L))))
val_data <- tuple(list(x_val, y_val))

x_test <- r_to_py(X_pca[test_idx, , drop = FALSE])
y_test <- r_to_py(np$reshape(y_pca[test_idx], tuple(list(length(test_idx), 1L))))

# Define model
model_pca <- keras$models$Sequential()
model_pca$add(keras$layers$Dense(units = 64L, activation = "relu", input_shape = shape))
model_pca$add(keras$layers$Dense(units = 32L, activation = "relu"))
model_pca$add(keras$layers$Dense(units = 4L, activation = "softmax"))  # 4 classes for workout type

# Compile
model_pca$compile(
  loss = "sparse_categorical_crossentropy",
  optimizer = keras$optimizers$Adam(),
  metrics = list("accuracy", "mae")
)

# Train
history_pca <- model_pca$fit(
  x = x_train,
  y = y_train,
  epochs = 150L,
  batch_size = 16L,
  validation_data = val_data
)

# Evaluate
pred_probs <- model_pca$predict(x_test)
pred_classes <- np$argmax(pred_probs, axis = 1L)
true_classes <- as.integer(py_to_r(y_test))

accuracy <- mean(pred_classes == true_classes)
mae_manual <- mean(abs(pred_classes - true_classes))

cat("PCA Test Accuracy:", round(accuracy * 100, 2), "%\n") #29.59%
cat("PCA Test MAE:", round(mae_manual, 4), "\n") # 1.3163
print(table(Predicted = pred_classes, Actual = true_classes))




















library(tidyverse)
library(GGally)
library(glmnet)
library(dplyr)
library(readxl)
library(reticulate)
library(keras)
library(MASS)

# Load data
df <- read.csv("gym_members_exercise_tracking.csv")

# Convert character variables to factors and then numeric
df$Workout_Type <- factor(df$Workout_Type, levels = c("Cardio", "HIIT", "Strength", "Yoga"), labels = c(1, 2, 3, 4))
df$Workout_Type <- as.numeric(df$Workout_Type)
df$Gender <- factor(df$Gender, levels = c("Female", "Male"), labels = c(1, 2))
df$Gender <- as.numeric(df$Gender)

# Calculate LBM, FFMI, and natural status
df$Fat_Mass <- df$Weight..kg. * (df$Fat_Percentage / 100)
df$LBM <- df$Weight..kg. - df$Fat_Mass
df$FFMI <- df$LBM / (df$Height..m.^2)

df <- df %>%
  mutate(natural_status = case_when(
    Gender == 2 & (LBM > 90 | FFMI > 25) ~ "PEDs",
    Gender == 2 & (LBM > 85 | FFMI > 23) ~ "Suspicious",
    Gender == 1 & (LBM > 70 | FFMI > 24) ~ "PEDs",
    Gender == 1 & (LBM > 65 | FFMI > 22) ~ "Suspicious",
    TRUE ~ "Natural"
  ))

df$natural_status <- factor(df$natural_status, levels = c("PEDs", "Suspicious", "Natural"), labels = c(1, 2, 3))
df$natural_status <- as.numeric(df$natural_status)
df <- df %>% dplyr::select(-LBM, -FFMI)

# Drop unnecessary variables
df <- df %>% dplyr::select(-Calories_Burned, -Water_Intake..liters., -Experience_Level)
df
# Prepare data (no PCA)
x <- as.matrix(df[, c("Age", "Gender", "Weight..kg.", "Height..m.", "Max_BPM", "Avg_BPM",
                      "Resting_BPM", "Session_Duration..hours.", "Fat_Percentage",
                      "Workout_Frequency..days.week.", "natural_status", "Fat_Mass", "BMI")])



y <- df$Workout_Type

set.seed(13)
n <- nrow(x)
train_size <- floor(0.6 * n)
val_size <- floor(0.2 * n)
test_size <- n - train_size - val_size

indices <- sample(1:n)
train_idx <- indices[1:train_size]
val_idx <- indices[(train_size + 1):(train_size + val_size)]
test_idx <- indices[(train_size + val_size + 1):n]

keras <- import("keras")
np <- import("numpy")
tuple <- import_builtins()$tuple
shape <- tuple(list(ncol(x)))

x_train <- r_to_py(x[train_idx, , drop = FALSE])
y_train <- r_to_py(np$reshape(y[train_idx] - 1L, tuple(list(length(train_idx), 1L))))

x_val <- r_to_py(x[val_idx, , drop = FALSE])
y_val <- r_to_py(np$reshape(y[val_idx] - 1L, tuple(list(length(val_idx), 1L))))
val_data <- tuple(list(x_val, y_val))

x_test <- r_to_py(x[test_idx, , drop = FALSE])
y_test <- r_to_py(np$reshape(y[test_idx] - 1L, tuple(list(length(test_idx), 1L))))

# Define model
model <- keras$models$Sequential()
model$add(keras$layers$Dense(units = 64L, activation = "relu", input_shape = shape))
model$add(keras$layers$Dense(units = 32L, activation = "relu"))
model$add(keras$layers$Dense(units = 4L, activation = "softmax"))

# Compile
model$compile(
  loss = "sparse_categorical_crossentropy",
  optimizer = keras$optimizers$Adam(),
  metrics = list("accuracy", "mae")
)

# Train
history <- model$fit(
  x = x_train,
  y = y_train,
  epochs = 150L,
  batch_size = 16L,
  validation_data = val_data
)

# Evaluate
pred_probs <- model$predict(x_test)
pred_classes <- np$argmax(pred_probs, axis = 1L)
true_classes <- as.integer(py_to_r(y_test))

accuracy <- mean(pred_classes == true_classes)
mae_manual <- mean(abs(pred_classes - true_classes))

cat("Test Accuracy:", round(accuracy * 100, 2), "%\n")
cat("Test MAE:", round(mae_manual, 4), "\n")
print(table(Predicted = pred_classes, Actual = true_classes))


library(tidyverse)
library(GGally)
library(glmnet)
library(dplyr)
library(readxl)
library(reticulate)
library(keras)
library(MASS)

# Load data
df <- read.csv("gym_members_exercise_tracking.csv")

# Convert character variables to factors and then numeric
df$Workout_Type <- factor(df$Workout_Type, levels = c("Cardio", "HIIT", "Strength", "Yoga"), labels = c(1, 2, 3, 4))
df$Workout_Type <- as.numeric(df$Workout_Type)
df$Gender <- factor(df$Gender, levels = c("Female", "Male"), labels = c(1, 2))
df$Gender <- as.numeric(df$Gender)

# Calculate LBM, FFMI, and natural status
df$Fat_Mass <- df$Weight..kg. * (df$Fat_Percentage / 100)
df$LBM <- df$Weight..kg. - df$Fat_Mass
df$FFMI <- df$LBM / (df$Height..m.^2)

df <- df %>%
  mutate(natural_status = case_when(
    Gender == 2 & (LBM > 90 | FFMI > 25) ~ "PEDs",
    Gender == 2 & (LBM > 85 | FFMI > 23) ~ "Suspicious",
    Gender == 1 & (LBM > 70 | FFMI > 24) ~ "PEDs",
    Gender == 1 & (LBM > 65 | FFMI > 22) ~ "Suspicious",
    TRUE ~ "Natural"
  ))

df$natural_status <- factor(df$natural_status, levels = c("PEDs", "Suspicious", "Natural"), labels = c(1, 2, 3))
df$natural_status <- as.numeric(df$natural_status)
df <- df %>% dplyr::select(-LBM, -FFMI)


# Drop unnecessary variables
df <- df %>% dplyr::select(-Calories_Burned, -Water_Intake..liters., -Experience_Level)




# Prepare data for PCA â€” Exclude Workout_Type from X
x <- as.matrix(df[, c("Age", "Gender", "Weight..kg.", "Height..m.", "Max_BPM", "Avg_BPM",
                      "Resting_BPM", "Session_Duration..hours.", "Fat_Percentage",
                      "Workout_Frequency..days.week.", "Fat_Mass", "BMI")])

y <- df$natural_status  # Set dependent variable to Workout_Type

# PCA
pca_model <- prcomp(x, scale. = TRUE)
pca_model$rotation <- -pca_model$rotation
pca_model$x <- -pca_model$x

# Prepare PCA dataset
pca_data <- data.frame(pca_model$x[, 1:7])
pca_data$target <- y
pca_data$target <- as.factor(pca_data$target)
pca_data$target_index <- as.integer(pca_data$target) - 1

# Prepare data for keras classification
X_pca <- as.matrix(pca_data[, paste0("PC", 1:7)])
y_pca <- pca_data$target_index

set.seed(13)
n <- nrow(X_pca)
train_size <- floor(0.6 * n)
val_size <- floor(0.2 * n)
test_size <- n - train_size - val_size

indices <- sample(1:n)
train_idx <- indices[1:train_size]
val_idx <- indices[(train_size + 1):(train_size + val_size)]
test_idx <- indices[(train_size + val_size + 1):n]

keras <- import("keras")
np <- import("numpy")
tuple <- import_builtins()$tuple
shape <- tuple(list(ncol(X_pca)))

x_train <- r_to_py(X_pca[train_idx, , drop = FALSE])
y_train <- r_to_py(np$reshape(y_pca[train_idx], tuple(list(length(train_idx), 1L))))

x_val <- r_to_py(X_pca[val_idx, , drop = FALSE])
y_val <- r_to_py(np$reshape(y_pca[val_idx], tuple(list(length(val_idx), 1L))))
val_data <- tuple(list(x_val, y_val))

x_test <- r_to_py(X_pca[test_idx, , drop = FALSE])
y_test <- r_to_py(np$reshape(y_pca[test_idx], tuple(list(length(test_idx), 1L))))

# Define model
model_pca <- keras$models$Sequential()
model_pca$add(keras$layers$Dense(units = 64L, activation = "relu", input_shape = shape))
model_pca$add(keras$layers$Dense(units = 32L, activation = "relu"))
model_pca$add(keras$layers$Dense(units = 4L, activation = "softmax"))  # 4 classes for workout type

# Compile
model_pca$compile(
  loss = "sparse_categorical_crossentropy",
  optimizer = keras$optimizers$Adam(),
  metrics = list("accuracy", "mae")
)

# Train
history_natural <- model_pca$fit(
  x = x_train,
  y = y_train,
  epochs = 150L,
  batch_size = 16L,
  validation_data = val_data
)

# Evaluate
pred_probs <- model_pca$predict(x_test)
pred_classes <- np$argmax(pred_probs, axis = 1L)
true_classes <- as.integer(py_to_r(y_test))

accuracy <- mean(pred_classes == true_classes)
mae_manual <- mean(abs(pred_classes - true_classes))

cat("PCA Test Accuracy:", round(accuracy * 100, 2), "%\n") #29.59%
cat("PCA Test MAE:", round(mae_manual, 4), "\n") # 1.3163
print(table(Predicted = pred_classes, Actual = true_classes))
# Prepare data for keras classification
X_pca <- as.matrix(pca_data[, paste0("PC", 1:7)])
y_pca <- pca_data$target_index

set.seed(13)
n <- nrow(X_pca)
train_size <- floor(0.6 * n)
val_size <- floor(0.2 * n)
test_size <- n - train_size - val_size

indices <- sample(1:n)
train_idx <- indices[1:train_size]
val_idx <- indices[(train_size + 1):(train_size + val_size)]
test_idx <- indices[(train_size + val_size + 1):n]

keras <- import("keras")
np <- import("numpy")
tuple <- import_builtins()$tuple
shape <- tuple(list(ncol(X_pca)))

x_train <- r_to_py(X_pca[train_idx, , drop = FALSE])
y_train <- r_to_py(np$reshape(y_pca[train_idx], tuple(list(length(train_idx), 1L))))

x_val <- r_to_py(X_pca[val_idx, , drop = FALSE])
y_val <- r_to_py(np$reshape(y_pca[val_idx], tuple(list(length(val_idx), 1L))))
val_data <- tuple(list(x_val, y_val))

x_test <- r_to_py(X_pca[test_idx, , drop = FALSE])
y_test <- r_to_py(np$reshape(y_pca[test_idx], tuple(list(length(test_idx), 1L))))

# Define model
model_pca <- keras$models$Sequential()
model_pca$add(keras$layers$Dense(units = 64L, activation = "relu", input_shape = shape))
model_pca$add(keras$layers$Dense(units = 32L, activation = "relu"))
model_pca$add(keras$layers$Dense(units = 4L, activation = "softmax"))  # 4 classes for workout type

# Compile
model_pca$compile(
  loss = "sparse_categorical_crossentropy",
  optimizer = keras$optimizers$Adam(),
  metrics = list("accuracy", "mae")
)

# Train
history_pca <- model_pca$fit(
  x = x_train,
  y = y_train,
  epochs = 150L,
  batch_size = 16L,
  validation_data = val_data
)

# Evaluate
pred_probs <- model_pca$predict(x_test)
pred_classes <- np$argmax(pred_probs, axis = 1L)
true_classes <- as.integer(py_to_r(y_test))

accuracy <- mean(pred_classes == true_classes)
mae_manual <- mean(abs(pred_classes - true_classes))

cat("PCA Test Accuracy:", round(accuracy * 100, 2), "%\n") 
cat("PCA Test MAE:", round(mae_manual, 4), "\n") 
print(table(Predicted = pred_classes, Actual = true_classes))


library(tidyverse)
library(GGally)
library(glmnet)
library(dplyr)
library(readxl)
library(reticulate)
library(keras)
library(MASS)

# Load data
df <- read.csv("gym_members_exercise_tracking.csv")

# Convert character variables to factors and then numeric
df$Workout_Type <- factor(df$Workout_Type, levels = c("Cardio", "HIIT", "Strength", "Yoga"), labels = c(1, 2, 3, 4))
df$Workout_Type <- as.numeric(df$Workout_Type)
df$Gender <- factor(df$Gender, levels = c("Female", "Male"), labels = c(1, 2))
df$Gender <- as.numeric(df$Gender)

# Calculate LBM, FFMI, and natural status
df$Fat_Mass <- df$Weight..kg. * (df$Fat_Percentage / 100)
df$LBM <- df$Weight..kg. - df$Fat_Mass
df$FFMI <- df$LBM / (df$Height..m.^2)

df <- df %>%
  mutate(natural_status = case_when(
    Gender == 2 & (LBM > 90 | FFMI > 25) ~ "PEDs",
    Gender == 2 & (LBM > 85 | FFMI > 23) ~ "Suspicious",
    Gender == 1 & (LBM > 70 | FFMI > 24) ~ "PEDs",
    Gender == 1 & (LBM > 65 | FFMI > 22) ~ "Suspicious",
    TRUE ~ "Natural"
    
    library(tidyverse)
    library(GGally)
    library(glmnet)
    library(dplyr)
    library(readxl)
    library(reticulate)
    library(keras)
    library(MASS)
    
    # Load data
    df <- read.csv("gym_members_exercise_tracking.csv")
    
    # Convert character variables to factors and then numeric
    df$Workout_Type <- factor(df$Workout_Type, levels = c("Cardio", "HIIT", "Strength", "Yoga"), labels = c(1, 2, 3, 4))
    df$Workout_Type <- as.numeric(df$Workout_Type)
    df$Gender <- factor(df$Gender, levels = c("Female", "Male"), labels = c(1, 2))
    df$Gender <- as.numeric(df$Gender)
    
    # Calculate LBM, FFMI, and natural status
    df$Fat_Mass <- df$Weight..kg. * (df$Fat_Percentage / 100)
    df$LBM <- df$Weight..kg. - df$Fat_Mass
    df$FFMI <- df$LBM / (df$Height..m.^2)
    
    df <- df %>%
      mutate(natural_status = case_when(
        Gender == 2 & (LBM > 90 | FFMI > 25) ~ "PEDs",
        Gender == 2 & (LBM > 85 | FFMI > 23) ~ "Suspicious",
        Gender == 1 & (LBM > 70 | FFMI > 24) ~ "PEDs",
        Gender == 1 & (LBM > 65 | FFMI > 22) ~ "Suspicious",
        TRUE ~ "Natural"
      ))
    
    df$natural_status <- factor(df$natural_status, levels = c("PEDs", "Suspicious", "Natural"), labels = c(1, 2, 3))
    df$natural_status <- as.numeric(df$natural_status)
    df <- df %>% dplyr::select(-LBM, -FFMI)
    
    # Drop unnecessary variables
    df <- df %>% dplyr::select(-Calories_Burned, -Water_Intake..liters., -Experience_Level)
    df
    # Prepare data (no PCA)
    x <- as.matrix(df[, c("Age", "Gender", "Weight..kg.", "Height..m.", "Max_BPM", "Avg_BPM",
                          "Resting_BPM", "Session_Duration..hours.", "Fat_Percentage",
                          "Workout_Frequency..days.week.", "natural_status", "Fat_Mass", "BMI")])
    
    
    
    y <- df$Workout_Type
    
    set.seed(13)
    n <- nrow(x)
    train_size <- floor(0.6 * n)
    val_size <- floor(0.2 * n)
    test_size <- n - train_size - val_size
    
    indices <- sample(1:n)
    train_idx <- indices[1:train_size]
    val_idx <- indices[(train_size + 1):(train_size + val_size)]
    test_idx <- indices[(train_size + val_size + 1):n]
    
    keras <- import("keras")
    np <- import("numpy")
    tuple <- import_builtins()$tuple
    shape <- tuple(list(ncol(x)))
    
    x_train <- r_to_py(x[train_idx, , drop = FALSE])
    y_train <- r_to_py(np$reshape(y[train_idx] - 1L, tuple(list(length(train_idx), 1L))))
    
    x_val <- r_to_py(x[val_idx, , drop = FALSE])
    y_val <- r_to_py(np$reshape(y[val_idx] - 1L, tuple(list(length(val_idx), 1L))))
    val_data <- tuple(list(x_val, y_val))
    
    x_test <- r_to_py(x[test_idx, , drop = FALSE])
    y_test <- r_to_py(np$reshape(y[test_idx] - 1L, tuple(list(length(test_idx), 1L))))
    
    # Define model
    model <- keras$models$Sequential()
    model$add(keras$layers$Dense(units = 64L, activation = "relu", input_shape = shape))
    model$add(keras$layers$Dense(units = 32L, activation = "relu"))
    model$add(keras$layers$Dense(units = 4L, activation = "softmax"))
    
    # Compile
    model$compile(
      loss = "sparse_categorical_crossentropy",
      optimizer = keras$optimizers$Adam(),
      metrics = list("accuracy", "mae")
    )
    
    # Train
    history <- model$fit(
      x = x_train,
      y = y_train,
      epochs = 150L,
      batch_size = 16L,
      validation_data = val_data
    )
    
    # Evaluate
    pred_probs <- model$predict(x_test)
    pred_classes <- np$argmax(pred_probs, axis = 1L)
    true_classes <- as.integer(py_to_r(y_test))
    
    accuracy <- mean(pred_classes == true_classes)
    mae_manual <- mean(abs(pred_classes - true_classes))
    
    cat("Test Accuracy:", round(accuracy * 100, 2), "%\n")
    cat("Test MAE:", round(mae_manual, 4), "\n")
    print(table(Predicted = pred_classes, Actual = true_classes))
    
    # Evaluate
    pred_probs <- model$predict(x_test)
    pred_classes <- np$argmax(pred_probs, axis = 1L)
    true_classes <- as.integer(py_to_r(y_test))
    
    accuracy <- mean(pred_classes == true_classes)
    mae_manual <- mean(abs(pred_classes - true_classes))
    
    cat("Test Accuracy:", round(accuracy * 100, 2), "%\n")
    cat("Test MAE:", round(mae_manual, 4), "\n")
    print(table(Predicted = pred_classes, Actual = true_classes))
    
    
    library(tidyverse)
    library(GGally)
    library(glmnet)
    library(dplyr)
    library(readxl)
    library(reticulate)
    library(keras)
    library(MASS)
    
    # Load data
    df <- read.csv("gym_members_exercise_tracking.csv")
    
    # Convert character variables to factors and then numeric
    df$Workout_Type <- factor(df$Workout_Type, levels = c("Cardio", "HIIT", "Strength", "Yoga"), labels = c(1, 2, 3, 4))
    df$Workout_Type <- as.numeric(df$Workout_Type)
    df$Gender <- factor(df$Gender, levels = c("Female", "Male"), labels = c(1, 2))
    df$Gender <- as.numeric(df$Gender)
    
    # Calculate LBM, FFMI, and natural status
    df$Fat_Mass <- df$Weight..kg. * (df$Fat_Percentage / 100)
    df$LBM <- df$Weight..kg. - df$Fat_Mass
    df$FFMI <- df$LBM / (df$Height..m.^2)
    
    df <- df %>%
      mutate(natural_status = case_when(
        Gender == 2 & (LBM > 90 | FFMI > 25) ~ "PEDs",
        Gender == 2 & (LBM > 85 | FFMI > 23) ~ "Suspicious",
        Gender == 1 & (LBM > 70 | FFMI > 24) ~ "PEDs",
        Gender == 1 & (LBM > 65 | FFMI > 22) ~ "Suspicious",
        TRUE ~ "Natural"
      ))
    
    df$natural_status <- factor(df$natural_status, levels = c("PEDs", "Suspicious", "Natural"), labels = c(1, 2, 3))
    df$natural_status <- as.numeric(df$natural_status)
    df <- df %>% dplyr::select(-LBM, -FFMI)
    
    # Drop unnecessary variables
    df <- df %>% dplyr::select(-Calories_Burned, -Water_Intake..liters., -Experience_Level)
    df
    # Prepare data (no PCA)
    x <- as.matrix(df[, c("Age", "Gender", "Weight..kg.", "Height..m.", "Max_BPM", "Avg_BPM",
                          "Resting_BPM", "Session_Duration..hours.", "Fat_Percentage",
                          "Workout_Frequency..days.week.", "Fat_Mass", "BMI")])
    
    
    
    
    y <- df$natural_status
    
    set.seed(13)
    n <- nrow(x)
    train_size <- floor(0.6 * n)
    val_size <- floor(0.2 * n)
    test_size <- n - train_size - val_size
    
    indices <- sample(1:n)
    train_idx <- indices[1:train_size]
    val_idx <- indices[(train_size + 1):(train_size + val_size)]
    test_idx <- indices[(train_size + val_size + 1):n]
    
    keras <- import("keras")
    np <- import("numpy")
    tuple <- import_builtins()$tuple
    shape <- tuple(list(ncol(x)))
    
    x_train <- r_to_py(x[train_idx, , drop = FALSE])
    y_train <- r_to_py(np$reshape(y[train_idx] - 1L, tuple(list(length(train_idx), 1L))))
    
    x_val <- r_to_py(x[val_idx, , drop = FALSE])
    y_val <- r_to_py(np$reshape(y[val_idx] - 1L, tuple(list(length(val_idx), 1L))))
    val_data <- tuple(list(x_val, y_val))
    
    x_test <- r_to_py(x[test_idx, , drop = FALSE])
    y_test <- r_to_py(np$reshape(y[test_idx] - 1L, tuple(list(length(test_idx), 1L))))
    
    # Define model
    model <- keras$models$Sequential()
    model$add(keras$layers$Dense(units = 64L, activation = "relu", input_shape = shape))
    model$add(keras$layers$Dense(units = 32L, activation = "relu"))
    model$add(keras$layers$Dense(units = 4L, activation = "softmax"))
    
    # Compile
    model$compile(
      loss = "sparse_categorical_crossentropy",
      optimizer = keras$optimizers$Adam(),
      metrics = list("accuracy", "mae")
    )
    
    # Train
    history <- model$fit(
      x = x_train,
      y = y_train,
      epochs = 150L,
      batch_size = 16L,
      validation_data = val_data
    )
    
    # Evaluate
    pred_probs <- model$predict(x_test)
    pred_classes <- np$argmax(pred_probs, axis = 1L)
    true_classes <- as.integer(py_to_r(y_test))
    
    accuracy <- mean(pred_classes == true_classes)
    mae_manual <- mean(abs(pred_classes - true_classes))
    
    cat("Test Accuracy:", round(accuracy * 100, 2), "%\n")
    cat("Test MAE:", round(mae_manual, 4), "\n")
    print(table(Predicted = pred_classes, Actual = true_classes))
    
    install.packages("MASS")  # if not already installed
    library(MASS)
    library(dplyr)
    
    # Defining formula of features
    lda_formula <- natural_status ~ Age + Gender + Weight..kg. + Height..m. + 
      Max_BPM + Avg_BPM + Resting_BPM + 
      Session_Duration..hours. + Fat_Percentage + 
      Workout_Frequency..days.week. + Fat_Mass + BMI
    
    set.seed(13)
    train_indices <- sample(1:nrow(df), size = 0.7 * nrow(df))  # 70% train
    df_train <- df[train_indices, ]
    df_test <- df[-train_indices, ]
    
    lda_model <- lda(lda_formula, data = df_train)
    
    lda_pred <- predict(lda_model, newdata = df_test)
    
    confusion_matrix <- table(Predicted = lda_pred$class, Actual = df_test$natural_status)
    print(confusion_matrix)
    
    accuracy <- mean(lda_pred$class == df_test$natural_status)
    print(paste("Test Accuracy:", round(accuracy * 100, 2), "%")) 
    #94.51
    
    
    # PCA classification using natural_status as target
    x <- as.matrix(df[, c("Age", "Gender", "Weight..kg.", "Height..m.", "Max_BPM", "Avg_BPM",
                          "Resting_BPM", "Session_Duration..hours.", "Fat_Percentage",
                          "Workout_Frequency..days.week.", "Fat_Mass", "BMI")])
    y <- df$natural_status - 1L  # Convert to 0-indexed
    
    # PCA
    pca_model <- prcomp(x, scale. = TRUE)
    pca_model$rotation <- -pca_model$rotation
    pca_model$x <- -pca_model$x
    
    # Prepare PCA dataset
    pca_data <- data.frame(pca_model$x[, 1:7])
    pca_data$target <- y
    
    set.seed(13)
    n <- nrow(pca_data)
    train_size <- floor(0.7 * n)
    train_indices <- sample(1:n, size = train_size)
    
    df_train <- pca_data[train_indices, ]
    df_test <- pca_data[-train_indices, ]
    
    # LDA with PCA features
    lda_model <- lda(target ~ ., data = df_train)
    lda_pred <- predict(lda_model, newdata = df_test)
    
    confusion_matrix <- table(Predicted = lda_pred$class, Actual = df_test$target)
    print(confusion_matrix)
    
    accuracy <- mean(lda_pred$class == df_test$target)
    print(paste("Test Accuracy:", round(accuracy * 100, 2), "%"))
    
    
    
    install.packages("MASS")  # if not already installed
    library(MASS)
    library(dplyr)
    
    # Defining formula of features
    lda_formula <- Workout_Type ~ Age + Gender + Weight..kg. + Height..m. + 
      Max_BPM + Avg_BPM + Resting_BPM + 
      Session_Duration..hours. + Fat_Percentage + 
      Workout_Frequency..days.week. + Fat_Mass + BMI
    
    set.seed(13)
    train_indices <- sample(1:nrow(df), size = 0.7 * nrow(df))  # 70% train
    df_train <- df[train_indices, ]
    df_test <- df[-train_indices, ]
    
    lda_model <- lda(lda_formula, data = df_train)
    
    lda_pred <- predict(lda_model, newdata = df_test)
    
    confusion_matrix <- table(Predicted = lda_pred$class, Actual = df_test$Workout_Type)
    print(confusion_matrix)
    
    accuracy <- mean(lda_pred$class == df_test$Workout_Type)
    print(paste("Test Accuracy:", round(accuracy * 100, 2), "%")) #25%
    
    
    
    set.seed(13)
    train_indices <- sample(1:nrow(df), size = 0.7 * nrow(df))  # 70% train
    df_train <- df[train_indices, ]
    df_test <- df[-train_indices, ]
    
    lda_model <- lda(lda_formula, data = df_train)
    
    lda_pred <- predict(lda_model, newdata = df_test)
    
    confusion_matrix <- table(Predicted = lda_pred$class, Actual = df_test$natural_status)
    print(confusion_matrix)
    
    accuracy <- mean(lda_pred$class == df_test$natural_status)
    print(paste("Test Accuracy:", round(accuracy * 100, 2), "%")) 
    
    
    
